{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6807266354549091, w_module: 1.86503565781866\n",
      "iter: 10, loss: 0.6047988783795631, w_module: 1.8481489888845297\n",
      "iter: 20, loss: 0.5660832091387538, w_module: 1.8416649857462604\n",
      "iter: 30, loss: 0.5309314279730247, w_module: 1.852628762958914\n",
      "iter: 40, loss: 0.49873990315881916, w_module: 1.8798975391623567\n",
      "iter: 50, loss: 0.4692664495478231, w_module: 1.9209134420153002\n",
      "iter: 60, loss: 0.4422816596337917, w_module: 1.973036438941622\n",
      "iter: 70, loss: 0.4175668558824795, w_module: 2.0338455872138264\n",
      "iter: 80, loss: 0.3949164282672946, w_module: 2.101235926921896\n",
      "iter: 90, loss: 0.3741392267477511, w_module: 2.1734405660231073\n",
      "iter: 100, loss: 0.3550591551622385, w_module: 2.249013982747483\n",
      "iter: 110, loss: 0.33751516416979, w_module: 2.3267961857247195\n",
      "iter: 120, loss: 0.32136081353797313, w_module: 2.405870153248784\n",
      "iter: 130, loss: 0.30646354486334987, w_module: 2.4855197670708287\n",
      "iter: 140, loss: 0.29270377665622754, w_module: 2.5651918359489785\n",
      "iter: 150, loss: 0.27997390696010677, w_module: 2.6444635594027077\n",
      "iter: 160, loss: 0.26817728569910904, w_module: 2.7230155419037834\n",
      "iter: 170, loss: 0.2572272001987447, w_module: 2.8006098703921802\n",
      "iter: 180, loss: 0.2470459026432581, w_module: 2.8770725425450023\n",
      "iter: 190, loss: 0.2375636971348856, w_module: 2.9522795034752236\n",
      "iter: 200, loss: 0.22871809590403383, w_module: 3.026145610160481\n",
      "iter: 210, loss: 0.22045304847935793, w_module: 3.0986159385953096\n",
      "iter: 220, loss: 0.21271824371709563, w_module: 3.169658949317714\n",
      "iter: 230, loss: 0.20546848204891383, w_module: 3.239261119331016\n",
      "iter: 240, loss: 0.19866311376522536, w_module: 3.3074227276783668\n",
      "iter: 250, loss: 0.19226553831674803, w_module: 3.3741545473463113\n",
      "iter: 260, loss: 0.18624275927137954, w_module: 3.439475248947205\n",
      "iter: 270, loss: 0.1805649895436243, w_module: 3.5034093635923687\n",
      "iter: 280, loss: 0.17520530170196286, w_module: 3.565985685435037\n",
      "iter: 290, loss: 0.17013931847203295, w_module: 3.6272360202793323\n",
      "iter: 300, loss: 0.16534493893221427, w_module: 3.6871942069037673\n",
      "iter: 310, loss: 0.16080209630385894, w_module: 3.745895353552433\n",
      "iter: 320, loss: 0.15649254364519968, w_module: 3.8033752443787776\n",
      "iter: 330, loss: 0.15239966414971054, w_module: 3.8596698802551885\n",
      "iter: 340, loss: 0.1485083031168994, w_module: 3.914815125888402\n",
      "iter: 350, loss: 0.1448046190012263, w_module: 3.968846441074419\n",
      "iter: 360, loss: 0.14127595125119785, w_module: 4.0217986785501445\n",
      "iter: 370, loss: 0.13791070292575738, w_module: 4.073705934533657\n",
      "iter: 380, loss: 0.13469823632016775, w_module: 4.12460144090855\n",
      "iter: 390, loss: 0.13162878005067505, w_module: 4.17451749026865\n",
      "iter: 400, loss: 0.1286933462386804, w_module: 4.223485386828482\n",
      "iter: 410, loss: 0.12588365660340467, w_module: 4.2715354176234905\n",
      "iter: 420, loss: 0.12319207641954594, w_module: 4.318696839551391\n",
      "iter: 430, loss: 0.12061155542551866, w_module: 4.364997878703581\n",
      "iter: 440, loss: 0.11813557488069022, w_module: 4.410465739151573\n",
      "iter: 450, loss: 0.11575810006855622, w_module: 4.455126618925515\n",
      "iter: 460, loss: 0.11347353762879621, w_module: 4.499005731379726\n",
      "iter: 470, loss: 0.11127669717620334, w_module: 4.542127330507203\n",
      "iter: 480, loss: 0.10916275672998427, w_module: 4.584514739059417\n",
      "iter: 490, loss: 0.10712723153411047, w_module: 4.6261903785643\n",
      "iter: 500, loss: 0.10516594589934522, w_module: 4.66717580052534\n",
      "iter: 510, loss: 0.10327500774121728, w_module: 4.707491718237644\n",
      "iter: 520, loss: 0.10145078552638004, w_module: 4.7471580387797605\n",
      "iter: 530, loss: 0.09968988737320025, w_module: 4.78619389483896\n",
      "iter: 540, loss: 0.09798914208168316, w_module: 4.824617676107175\n",
      "iter: 550, loss: 0.09634558189349451, w_module: 4.862447060048657\n",
      "iter: 560, loss: 0.0947564268053612, w_module: 4.899699041891622\n",
      "iter: 570, loss: 0.093219070278911, w_module: 4.936389963737138\n",
      "iter: 580, loss: 0.0917310662074153, w_module: 4.972535542711247\n",
      "iter: 590, loss: 0.09029011701521598, w_module: 5.008150898112314\n",
      "iter: 600, loss: 0.08889406277912253, w_module: 5.043250577526276\n",
      "iter: 610, loss: 0.08754087127298131, w_module: 5.077848581898619\n",
      "iter: 620, loss: 0.0862286288471493, w_module: 5.111958389564642\n",
      "iter: 630, loss: 0.08495553206391399, w_module: 5.145592979249359\n",
      "iter: 640, loss: 0.08371988001814948, w_module: 5.17876485205591\n",
      "iter: 650, loss: 0.0825200672798088, w_module: 5.211486052466941\n",
      "iter: 660, loss: 0.08135457740134236, w_module: 5.243768188387556\n",
      "iter: 670, loss: 0.08022197693889695, w_module: 5.275622450261375\n",
      "iter: 680, loss: 0.07912090994128188, w_module: 5.307059629293139\n",
      "iter: 690, loss: 0.07805009286525623, w_module: 5.338090134812519\n",
      "iter: 700, loss: 0.07700830987976583, w_module: 5.368724010814337\n",
      "iter: 710, loss: 0.07599440852539532, w_module: 5.398970951710509\n",
      "iter: 720, loss: 0.0750072956985513, w_module: 5.428840317328728\n",
      "iter: 730, loss: 0.07404593393279954, w_module: 5.458341147192355\n",
      "iter: 740, loss: 0.07310933795238438, w_module: 5.487482174115176\n",
      "iter: 750, loss: 0.07219657147529357, w_module: 5.5162718371437505\n",
      "iter: 760, loss: 0.07130674424532708, w_module: 5.544718293879034\n",
      "iter: 770, loss: 0.07043900927451288, w_module: 5.572829432207747\n",
      "iter: 780, loss: 0.06959256027890548, w_module: 5.600612881472889\n",
      "iter: 790, loss: 0.0687666292923296, w_module: 5.628076023111421\n",
      "iter: 800, loss: 0.0679604844440046, w_module: 5.655226000786077\n",
      "iter: 810, loss: 0.06717342788722715, w_module: 5.682069730036912\n",
      "iter: 820, loss: 0.0664047938674085, w_module: 5.708613907477059\n",
      "iter: 830, loss: 0.06565394691877607, w_module: 5.734865019555978\n",
      "iter: 840, loss: 0.06492028017996579, w_module: 5.760829350912296\n",
      "iter: 850, loss: 0.06420321381955932, w_module: 5.786512992337291\n",
      "iter: 860, loss: 0.06350219356337615, w_module: 5.811921848368939\n",
      "iter: 870, loss: 0.06281668931600998, w_module: 5.837061644535446\n",
      "iter: 880, loss: 0.06214619386972086, w_module: 5.861937934266181\n",
      "iter: 890, loss: 0.06149022169435737, w_module: 5.886556105487031\n",
      "iter: 900, loss: 0.06084830780249567, w_module: 5.910921386916216\n",
      "iter: 910, loss: 0.06022000668445109, w_module: 5.935038854075815\n",
      "iter: 920, loss: 0.05960489130824031, w_module: 5.958913435033435\n",
      "iter: 930, loss: 0.05900255217996428, w_module: 5.982549915887605\n",
      "iter: 940, loss: 0.05841259646043255, w_module: 6.005952946009884\n",
      "iter: 950, loss: 0.05783464713417729, w_module: 6.029127043055798\n",
      "iter: 960, loss: 0.05726834222729812, w_module: 6.052076597756256\n",
      "iter: 970, loss: 0.05671333407085249, w_module: 6.074805878500292\n",
      "iter: 980, loss: 0.056169288606752674, w_module: 6.0973190357195275\n",
      "iter: 990, loss: 0.0556358847333592, w_module: 6.1196201060841\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object\n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "_,y_pred = LR.predict(x_train)\n",
    "# TODO: compute the accuracy\n",
    "train_acc = np.sum(y_train == y_pred) / y_train.shape[0]\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "_,y_pred_test = LR.predict(x_test)\n",
    "test_acc = np.sum(y_test == y_pred_test) / y_test.shape[0]\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.7626338792157056, w_module: 1.1666479112787131\n",
      "iter: 10, loss: 0.5512105917124799, w_module: 1.0059428772396901\n",
      "iter: 20, loss: 0.5032670058998098, w_module: 1.043507909594384\n",
      "iter: 30, loss: 0.4728857424911759, w_module: 1.1300133740395049\n",
      "iter: 40, loss: 0.44556921164099994, w_module: 1.23224679432182\n",
      "iter: 50, loss: 0.42068862324445283, w_module: 1.3417235335754876\n",
      "iter: 60, loss: 0.3979983804556591, w_module: 1.4544396438519054\n",
      "iter: 70, loss: 0.3772804938548322, w_module: 1.567975460189859\n",
      "iter: 80, loss: 0.35833700909365235, w_module: 1.6807962148313718\n",
      "iter: 90, loss: 0.3409891405975292, w_module: 1.7919296223532153\n",
      "iter: 100, loss: 0.3250761861747587, w_module: 1.9007712502048357\n",
      "iter: 110, loss: 0.31045421008900465, w_module: 2.006959637602104\n",
      "iter: 120, loss: 0.2969946166293214, w_module: 2.110294839991969\n",
      "iter: 130, loss: 0.2845827049256945, w_module: 2.210684709626059\n",
      "iter: 140, loss: 0.2731162653454001, w_module: 2.3081090719969226\n",
      "iter: 150, loss: 0.26250425467378685, w_module: 2.4025955745314724\n",
      "iter: 160, loss: 0.2526655706399038, w_module: 2.4942032436490913\n",
      "iter: 170, loss: 0.243527934819772, w_module: 2.583011199102756\n",
      "iter: 180, loss: 0.23502688527001975, w_module: 2.669110862723642\n",
      "iter: 190, loss: 0.22710487536673463, w_module: 2.752600562711369\n",
      "iter: 200, loss: 0.21971047243159197, w_module: 2.833581797327036\n",
      "iter: 210, loss: 0.21279764820310237, w_module: 2.91215665833757\n",
      "iter: 220, loss: 0.20632515260267276, w_module: 2.988426070923521\n",
      "iter: 230, loss: 0.20025596222859002, w_module: 3.0624886115648495\n",
      "iter: 240, loss: 0.19455679535821763, w_module: 3.134439736577902\n",
      "iter: 250, loss: 0.18919768579315388, w_module: 3.2043713028788403\n",
      "iter: 260, loss: 0.1841516085387796, w_module: 3.2723712965317393\n",
      "iter: 270, loss: 0.17939415100000478, w_module: 3.3385237084953925\n",
      "iter: 280, loss: 0.17490322405600192, w_module: 3.4029085138835007\n",
      "iter: 290, loss: 0.17065880802265052, w_module: 3.465601723124369\n",
      "iter: 300, loss: 0.16664272910839223, w_module: 3.526675482090831\n",
      "iter: 310, loss: 0.16283846251099818, w_module: 3.5861982045584315\n",
      "iter: 320, loss: 0.1592309587881036, w_module: 3.6442347249260796\n",
      "iter: 330, loss: 0.15580649056498216, w_module: 3.7008464624789195\n",
      "iter: 340, loss: 0.15255251702238187, w_module: 3.7560915909274746\n",
      "iter: 350, loss: 0.14945756393965026, w_module: 3.8100252087621427\n",
      "iter: 360, loss: 0.1465111173584985, w_module: 3.8626995072914836\n",
      "iter: 370, loss: 0.1437035291852397, w_module: 3.9141639342121017\n",
      "iter: 380, loss: 0.14102593326861013, w_module: 3.9644653512785792\n",
      "iter: 390, loss: 0.1384701706804413, w_module: 4.013648185170903\n",
      "iter: 400, loss: 0.13602872309121278, w_module: 4.0617545710432195\n",
      "iter: 410, loss: 0.1336946532751998, w_module: 4.108824488518122\n",
      "iter: 420, loss: 0.13146155190348321, w_module: 4.1548958900919\n",
      "iter: 430, loss: 0.12932348989008233, w_module: 4.200004822058483\n",
      "iter: 440, loss: 0.12727497564916612, w_module: 4.24418553815823\n",
      "iter: 450, loss: 0.1253109167016436, w_module: 4.287470606223539\n",
      "iter: 460, loss: 0.1234265851391265, w_module: 4.32989100813487\n",
      "iter: 470, loss: 0.12161758651376077, w_module: 4.371476233424717\n",
      "iter: 480, loss: 0.11987983177499556, w_module: 4.4122543668779315\n",
      "iter: 490, loss: 0.11820951192008676, w_module: 4.452252170478375\n",
      "iter: 500, loss: 0.11660307506495318, w_module: 4.4914951600467266\n",
      "iter: 510, loss: 0.11505720567671535, w_module: 4.530007676904523\n",
      "iter: 520, loss: 0.11356880573954477, w_module: 4.567812954886657\n",
      "iter: 530, loss: 0.11213497765192745, w_module: 4.604933183009688\n",
      "iter: 540, loss: 0.11075300867661131, w_module: 4.641389564087403\n",
      "iter: 550, loss: 0.10942035678480005, w_module: 4.6772023695685165\n",
      "iter: 560, loss: 0.10813463775395966, w_module: 4.71239099085489\n",
      "iter: 570, loss: 0.1068936133942378, w_module: 4.746973987342298\n",
      "iter: 580, loss: 0.10569518079224796, w_module: 4.780969131409969\n",
      "iter: 590, loss: 0.1045373624730796, w_module: 4.8143934505698756\n",
      "iter: 600, loss: 0.10341829739207134, w_module: 4.847263266972262\n",
      "iter: 610, loss: 0.1023362326773121, w_module: 4.87959423445018\n",
      "iter: 620, loss: 0.101289516052167, w_module: 4.911401373272823\n",
      "iter: 630, loss: 0.1002765888745007, w_module: 4.942699102765372\n",
      "iter: 640, loss: 0.09929597973580927, w_module: 4.973501271941666\n",
      "iter: 650, loss: 0.09834629856926969, w_module: 5.003821188285457\n",
      "iter: 660, loss: 0.097426231220872, w_module: 5.03367164480615\n",
      "iter: 670, loss: 0.09653453444238308, w_module: 5.063064945485765\n",
      "iter: 680, loss: 0.09567003126897325, w_module: 5.0920129292253735\n",
      "iter: 690, loss: 0.0948316067479788, w_module: 5.120526992391407\n",
      "iter: 700, loss: 0.09401820398852412, w_module: 5.1486181100549056\n",
      "iter: 710, loss: 0.0932288205046315, w_module: 5.176296856010074\n",
      "iter: 720, loss: 0.0924625048270478, w_module: 5.203573421652261\n",
      "iter: 730, loss: 0.09171835336134478, w_module: 5.230457633789683\n",
      "iter: 740, loss: 0.09099550747194049, w_module: 5.256958971457909\n",
      "iter: 750, loss: 0.09029315077356244, w_module: 5.283086581801205\n",
      "iter: 760, loss: 0.08961050661336138, w_module: 5.308849295080246\n",
      "iter: 770, loss: 0.08894683572839854, w_module: 5.334255638861522\n",
      "iter: 780, loss: 0.08830143406459844, w_module: 5.359313851439858\n",
      "iter: 790, loss: 0.0876736307444887, w_module: 5.384031894541894\n",
      "iter: 800, loss: 0.08706278617216148, w_module: 5.408417465355004\n",
      "iter: 810, loss: 0.08646829026489573, w_module: 5.4324780079231\n",
      "iter: 820, loss: 0.08588956080178754, w_module: 5.4562207239478955\n",
      "iter: 830, loss: 0.08532604188055842, w_module: 5.479652583031587\n",
      "iter: 840, loss: 0.08477720247445773, w_module: 5.502780332394431\n",
      "iter: 850, loss: 0.08424253508184928, w_module: 5.525610506098518\n",
      "iter: 860, loss: 0.08372155446168844, w_module: 5.548149433806859\n",
      "iter: 870, loss: 0.08321379644865169, w_module: 5.570403249105042\n",
      "iter: 880, loss: 0.08271881684218885, w_module: 5.592377897410854\n",
      "iter: 890, loss: 0.08223619036422965, w_module: 5.6140791434956485\n",
      "iter: 900, loss: 0.08176550968069693, w_module: 5.635512578639674\n",
      "iter: 910, loss: 0.08130638448236216, w_module: 5.656683627442108\n",
      "iter: 920, loss: 0.08085844062092956, w_module: 5.6775975543052875\n",
      "iter: 930, loss: 0.08042131929655463, w_module: 5.698259469611288\n",
      "iter: 940, loss: 0.07999467629329463, w_module: 5.7186743356079495\n",
      "iter: 950, loss: 0.0795781812592572, w_module: 5.738846972020297\n",
      "iter: 960, loss: 0.07917151702845696, w_module: 5.758782061402357\n",
      "iter: 970, loss: 0.07877437898161559, w_module: 5.7784841542434116\n",
      "iter: 980, loss: 0.07838647444334472, w_module: 5.7979576738419\n",
      "iter: 990, loss: 0.07800752211334254, w_module: 5.817206920959341\n"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "_,y_pred = LR.predict(x_train)\n",
    "\n",
    "train_acc = np.sum(y_train == y_pred) / y_train.shape[0]\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "_,y_pred_test = LR.predict(x_test)\n",
    "test_acc = np.sum(y_test == y_pred_test) / y_test.shape[0]\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正则化对过大的w进行惩罚，防止w持续变大。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
